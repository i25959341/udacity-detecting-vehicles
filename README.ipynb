{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking Vehicles Using Machine Learning and Computer Vision\n",
    "# Udacity Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "### By Wonjun Lee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Overview\n",
    "\n",
    "This is a machine learning and computer vision project from Udacity.\n",
    "\n",
    "In this project, I developed a model that detects cars from a video stream and draw rectangles around the cars detected by the model.\n",
    "\n",
    "The labeled data for vehicle and non-vehicle examples to train your classifier come from a combination of the [**GTI vehicle image database**](http://www.gti.ssr.upm.es/data/Vehicle_database.html), the [**KITTI vision benchmark suite**](http://www.cvlibs.net/datasets/kitti/), and examples extracted from the project video itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "When training the model, **SVM (Support Vector Machine)** was used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features used for the model consist of a spatial binning, a color histogram, HOG (Historgram of Oriented Gradient).\n",
    "\n",
    "I used **YCrCb** color space for the features. All the images were first converted to **YCrCb** from **RGB** color space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Spatial Binning of Color\n",
    "\n",
    "The first part of the feature is the raw pixel values of an image.\n",
    "The image from training data is resized into **16 by 16** and converted into a vector using **ravel()**.\n",
    "\n",
    "*cv2.resize()* function from **OpenCV** library is used to resize the image.\n",
    "\n",
    "    # Define a function to compute binned color features  \n",
    "    def bin_spatial(img, size=(32, 32)):\n",
    "        # Use cv2.resize().ravel() to create the feature vector\n",
    "        features = cv2.resize(img, size).ravel() \n",
    "        # Return the feature vector\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Histograms of Color\n",
    "\n",
    "We can detect a car by looking at its colors. Color of an image provides important information that can help us to distinguish a car from non-car.\n",
    "\n",
    "**YCrCb** just like any other color spaces contains **3** color channels. Histograms of pixels from each channel are constructed with bin size equals 32.\n",
    "\n",
    "    # Define a function to compute color histogram features \n",
    "    # NEED TO CHANGE bins_range if reading .png files with mpimg!\n",
    "    def color_hist(img, nbins=32, bins_range=(0, 256)):\n",
    "        # Compute the histogram of the color channels separately\n",
    "        channel1_hist = np.histogram(img[:,:,0], bins=nbins, range=bins_range)\n",
    "        channel2_hist = np.histogram(img[:,:,1], bins=nbins, range=bins_range)\n",
    "        channel3_hist = np.histogram(img[:,:,2], bins=nbins, range=bins_range)\n",
    "        # Concatenate the histograms into a single feature vector\n",
    "        hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "        # Return the individual histograms, bin_centers and feature vector\n",
    "        return hist_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Histogram of Oriented Gradient (HOG)\n",
    "A car can be distinguished from a non-car by looking at its edges. **HOG** will compute the gradients from blocks of cells. Then, a histogram is constructed with these gradient values.\n",
    "\n",
    "I used **hog** function from **scikit-image**.\n",
    "\n",
    "    # Define a function to return HOG features and visualization\n",
    "    def get_hog_features(img, orient, pix_per_cell, cell_per_block, \n",
    "                            vis=False, feature_vec=True):\n",
    "        # Call with two outputs if vis==True\n",
    "        if vis == True:\n",
    "            features, hog_image = hog(img, orientations=orient, \n",
    "                                      pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                      cells_per_block=(cell_per_block, cell_per_block), \n",
    "                                      transform_sqrt=True, \n",
    "                                      visualise=vis, feature_vector=feature_vec)\n",
    "            return features, hog_image\n",
    "        # Otherwise call with one output\n",
    "        else:      \n",
    "            features = hog(img, orientations=orient, \n",
    "                           pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                           cells_per_block=(cell_per_block, cell_per_block), \n",
    "                           transform_sqrt=True, \n",
    "                           visualise=vis, feature_vector=feature_vec)\n",
    "            return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Constucting A Model\n",
    "\n",
    "After extracting features from all data, I used **SVM** to train the features.\n",
    "\n",
    "Before training the data, the data was normalized using **StandardScaler()** from **sklearn.preprocessing**.\n",
    "\n",
    "Then these normalized data were splitted into train and test sets.\n",
    "\n",
    "    # Combining car and notcar features\n",
    "    X = np.vstack((car_features, notcar_features)).astype(np.float64)                        \n",
    "    # Fit a per-column scaler\n",
    "    X_scaler = StandardScaler().fit(X)\n",
    "    # Apply the scaler to X\n",
    "    scaled_X = X_scaler.transform(X)\n",
    "\n",
    "    # Define the labels vector\n",
    "    y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "\n",
    "\n",
    "    # Split up data into randomized training and test sets\n",
    "    rand_state = np.random.randint(0, 100)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "        \n",
    "    # Use a linear SVC \n",
    "    svc = LinearSVC(max_iter=20000)\n",
    "    svc.fit(X_train, y_train)\n",
    "   \n",
    "### The Best Set of Parameters  \n",
    "After repeating this procedure many times with different sets of parameters, I found the set of parameters below worked the best.\n",
    "\n",
    "- **Spatial Binning of Color**: size = (16, 16)\n",
    "- **Histograms of Color**: nbins = 32\n",
    "- **Histogram of Oriented Gradient (HOG)**: orient = 8, pix_per_cell = 8, cell_per_block = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sliding Window Search\n",
    "\n",
    "Below is the set of windows that I used to detect the cars from video stream. Small windows are located at the center and as the size of windows gets larger, they become closer to the bottom of the image.\n",
    "![alt text](sliding_windows.png \"Sliding Windows\")\n",
    "### Code for Sliding Window Search\n",
    "    # Define a function that takes an image,\n",
    "    # start and stop positions in both x and y, \n",
    "    # window size (x and y dimensions),  \n",
    "    # and overlap fraction (for both x and y)\n",
    "    def slide_window(img, x_start_stop=[None, None], y_start_stop=[None, None], \n",
    "                        xy_window=(64, 64), xy_overlap=(0.5, 0.5)):\n",
    "        # If x and/or y start/stop positions not defined, set to image size\n",
    "        if x_start_stop[0] == None:\n",
    "            x_start_stop[0] = 0\n",
    "        if x_start_stop[1] == None:\n",
    "            x_start_stop[1] = img.shape[1]\n",
    "        if y_start_stop[0] == None:\n",
    "            y_start_stop[0] = 0\n",
    "        if y_start_stop[1] == None:\n",
    "            y_start_stop[1] = img.shape[0]\n",
    "        # Compute the span of the region to be searched    \n",
    "        xspan = x_start_stop[1] - x_start_stop[0]\n",
    "        yspan = y_start_stop[1] - y_start_stop[0]\n",
    "        # Compute the number of pixels per step in x/y\n",
    "        nx_pix_per_step = np.int(xy_window[0]*(1 - xy_overlap[0]))\n",
    "        ny_pix_per_step = np.int(xy_window[1]*(1 - xy_overlap[1]))\n",
    "        # Compute the number of windows in x/y\n",
    "        nx_windows = np.int(xspan/nx_pix_per_step) - 1\n",
    "        ny_windows = np.int(yspan/ny_pix_per_step) - 1\n",
    "        # Initialize a list to append window positions to\n",
    "        window_list = []\n",
    "        # Loop through finding x and y window positions\n",
    "        # Note: you could vectorize this step, but in practice\n",
    "        # you'll be considering windows one by one with your\n",
    "        # classifier, so looping makes sense\n",
    "        for ys in range(ny_windows):\n",
    "            for xs in range(nx_windows):\n",
    "                # Calculate window position\n",
    "                startx = xs*nx_pix_per_step + x_start_stop[0]\n",
    "                endx = startx + xy_window[0]\n",
    "                starty = ys*ny_pix_per_step + y_start_stop[0]\n",
    "                endy = starty + xy_window[1]\n",
    "\n",
    "                # Append window position to list\n",
    "                window_list.append(((startx, starty), (endx, endy)))\n",
    "        # Return the list of windows\n",
    "        return window_list\n",
    "\n",
    "### Using the function to construct windows\n",
    "    windows = slide_window(image, x_start_stop=[None, None], y_start_stop=[400, 500], \n",
    "                        xy_window=(96, 96), xy_overlap=(0.75, 0.75))\n",
    "    windows += slide_window(image, x_start_stop=[None, None], y_start_stop=[400, 500], \n",
    "                        xy_window=(144, 144), xy_overlap=(0.75, 0.75))\n",
    "    windows += slide_window(image, x_start_stop=[None, None], y_start_stop=[430, 550], \n",
    "                        xy_window=(192, 192), xy_overlap=(0.75, 0.75))\n",
    "    windows += slide_window(image, x_start_stop=[None, None], y_start_stop=[460, 580], \n",
    "                        xy_window=(192, 192), xy_overlap=(0.75, 0.75))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Cars From An Image\n",
    "\n",
    "The model predicts a label (car or notcar) from an image inside each window. If a car is detected, then a rectangle is drawn. Below are example images with boxes drawn by the model.\n",
    "![alt text](windows_detected.png \"Detected Windows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Windows With A Heatmap\n",
    "When a car is detected, multiple boxes are drawn on the car, so I used a **heatmap** to combine boxes into a single box.\n",
    "\n",
    "**create_heatmap** function will convert windows into a heatmap.\n",
    "    # Convert windows to heatmap numpy array.\n",
    "    def create_heatmap(windows, image_shape):\n",
    "        background = np.zeros(image_shape[:2])\n",
    "        for window in windows:\n",
    "            background[window[0][1]:window[1][1], window[0][0]:window[1][0]] += 1\n",
    "        return background\n",
    "\n",
    "After the heatmap is created, windows are constructed arond the continuous nonzero areas. I found that those pixels are usually **false positives**.\n",
    "\n",
    "    # find the nonzero areas from a heatmap and\n",
    "    # turn them to windows\n",
    "    def find_windows_from_heatmap(image):\n",
    "        hot_windows = []\n",
    "        # Set labels\n",
    "        labels = ndi.label(image)\n",
    "        # iterate through labels and find windows\n",
    "        for car_number in range(1, labels[1]+1):\n",
    "            # Find pixels with each car_number label value\n",
    "            nonzero = (labels[0] == car_number).nonzero()\n",
    "            # Identify x and y values of those pixels\n",
    "            nonzeroy = np.array(nonzero[0])\n",
    "            nonzerox = np.array(nonzero[1])\n",
    "            # Define a bounding box based on min/max x and y\n",
    "            bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "            hot_windows.append(bbox)\n",
    "        return hot_windows, labels[0]\n",
    "\n",
    "**ndi.label** will find nonzero areas and label them starting from 1 and set the background as 0.\n",
    "\n",
    "More details about a **label** function can be found [here](https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.ndimage.measurements.label.html).\n",
    "\n",
    "Below are example images. Blue boxes are combined boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image_with_heatmap.png \"Detected Windows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Video Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall performance of the model is pretty nice; however, there are some cases when the model does not detect the car from an image, which results in no boxes drawn on the image. I implemented an algorithm that uses the windows data from previous frames to predict the location of the windows if the model fails to draw rectangles on cars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class Window contains an array **probability**. This array is initialized by **initialize_center_box** function.\n",
    "    # Define a class to receive the characteristics of each line detection\n",
    "    class Window():\n",
    "        def __init__(self):\n",
    "            self.probability = []\n",
    "\n",
    "    # Create an array for the center and the radius of the boxes\n",
    "    def initialize_center_box(boxes):\n",
    "        result = []\n",
    "        for box in boxes:\n",
    "            center = find_center(box)\n",
    "            width, height = find_radius(box)\n",
    "            move = (0, 0) # movement of an object\n",
    "            result.append((center, width, height, move, 1))\n",
    "        return result\n",
    "\n",
    "Each item in the array contains 5 values.\n",
    "1. center: x and y coordinates of a center of a box\n",
    "2. width: A width of a box\n",
    "3. height: A height of a box\n",
    "4. move: Changes in x values and y values of a center of a box\n",
    "5. prob: This is the confidence level of the box. If the value is high then it will be likely that the box should be drawn even though the model doesn't detect anything in that area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then **average_boxes** function is used to compare new windows from current frame and old windows from previous frames. If only the boxes with high confidence value wil be drawn. In this project, I used confidence level equals 2.\n",
    "    # Compare the new boxes with boxes from previous frames.\n",
    "    def average_boxes(hot_windows, old_boxes, \n",
    "                      image_shape):\n",
    "        hot_boxes = initialize_center_box(hot_windows)\n",
    "        new_boxes = add_center_box(hot_boxes, old_boxes)\n",
    "        # print('new_boxes', new_boxes)\n",
    "        filtered_boxes = []\n",
    "        for new_box in new_boxes:\n",
    "            # Draw boxes only if the confidence level is above 2\n",
    "            if new_box[-1] > 2:\n",
    "                filtered_boxes.append(new_box)\n",
    "        new_windows = []\n",
    "        # convert center-width-height to lefttop-rightbottom format\n",
    "        for filtered_box in filtered_boxes:\n",
    "            new_center, new_width, new_height,new_move, new_prob = filtered_box\n",
    "            new_windows.append(((int(new_center[0]-new_width), int(new_center[1]-new_height)), \n",
    "                (int(new_center[0]+new_width), int(new_center[1]+new_height))))\n",
    "        # return the boxes with high confidence and new set of probability array\n",
    "        return new_windows, new_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Result\n",
    "\n",
    "Here's a link to my video result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Discussion\n",
    "\n",
    "1. **Briefly discuss any problems / issues you faced in your implementation of this project. Where will your pipeline likely fail? What could you do to make it more robust?**\n",
    "\n",
    "The video that I used for the project shows clear distinguishment between cars and a road. If the video contains a non car objects such as pedestrians, then the model will likely detect them as cars. The model will also fail if the video contains a lot of noise pixels. To improve the model, I will have to increase the size of training data for model to identify non car objects more confidently.\n",
    "\n",
    "I will work on improving **average_boxes** function and **find_windows_from_heatmap**. The model doesn't separate two cars very well when they are close together. After this, I will combine this pipeline with the pipeline from a previous project that detects lanes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
